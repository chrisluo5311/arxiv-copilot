import os
from openai import OpenAI
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# ------------------ Configuration ------------------ #
load_dotenv()
os.environ["TOKENIZERS_PARALLELISM"] = "false"
# o4-mini = 200,000 context window # 100,000 max output tokens
# gpt-3.5-turbo = 16,385 context window # 4,096 max output tokens
# gpt-4o = 128,000 context window # 16,384 max output tokens
# chatgpt-4o (currently used in ChatGPT) = 128,000 context window # 16,384 max output tokens
MODEL_NAME = "gpt-4o"
MAX_CONTEXT_TOKENS = 12_000           # leave room for the question & response
model = SentenceTransformer("all-MiniLM-L6-v2")
current_dir = os.path.dirname(os.path.abspath(__file__)) # /Users/luojidong/Á®ãÂºè/arxiv-copilot/scripts
chunk_dir = os.path.join(current_dir, "pdf_chunks") # /Users/luojidong/Á®ãÂºè/arxiv-copilot/scripts/pdf_chunks
upload_file_chunk_dir = os.path.join(current_dir, "upload_file_chunks") # /Users/luojidong/Á®ãÂºè/arxiv-copilot/scripts/pdf_chunks
# --------------------------------------------------- #

# Try chunk_size = 300‚Äì400, stride = 100‚Äì200
#  - smaller chunk_size for short question-answering
#  - larger chunk_size for summarization or multi-section questions
def load_and_chunk(arxiv_id, chunk_size=300, stride=100):
    path = os.path.join(chunk_dir, f"{arxiv_id}.txt") # /Users/luojidong/Á®ãÂºè/arxiv-copilot/scripts/pdf_chunks/0704.0001.txt
    if not os.path.exists(path):
        raise FileNotFoundError(f"arXiv file not found: {path}")
    with open(path, "r") as f:
        full_text = f.read()

    # Split into chunks of N words
    words = full_text.split()
    # Overlapping: [0,299], [100,399], [200,499], ...
    # Better continuity between chunks
    chunks = [" ".join(words[i:i + chunk_size]) for i in range(0, len(words) - chunk_size + 1, stride)]
    return chunks

def load_uploaded_file_and_chunk(file_name, chunk_size=300, stride=100):
    path = os.path.join(upload_file_chunk_dir, f"{file_name}") # /Users/luojidong/Á®ãÂºè/arxiv-copilot/scripts/pdf_chunks/0704.0001.txt
    if not os.path.exists(path):
        raise FileNotFoundError(f"uploaded file not found: {path}")
    with open(path, "r") as f:
        full_text = f.read()

    # Split into chunks of N words
    words = full_text.split()
    # Overlapping: [0,299], [100,399], [200,499], ...
    # Better continuity between chunks
    chunks = [" ".join(words[i:i + chunk_size]) for i in range(0, len(words) - chunk_size + 1, stride)]
    return chunks

def retrieve_top_chunks(query, chunks, top_k=10):
    if not chunks:
        raise ValueError("No chunks available for retrieval.")
    chunk_embeddings = model.encode(chunks)
    query_embedding = model.encode([query])
    similarities = cosine_similarity(query_embedding, chunk_embeddings)[0]
    top_indices = similarities.argsort()[::-1][:top_k]
    return [chunks[i] for i in top_indices]

def standalone_answer(query, chunks, model_name, base64_image, api_key=None):
    client = OpenAI(api_key=api_key)
    context = "\n\n".join(chunks)
    prompt = f"""You are a helpful AI assistant specialized in scientific research.
            You are given a user question and some context extracted from a scientific paper.
            
            Context:
            \"\"\"
            {context}
            \"\"\"
            
            Question: {query}
            Answer:"""

    response = client.chat.completions.create(
        model=model_name,
        temperature=0.3,
        messages=[
            {
                "role": "developer",
                "content": "You are a research assistant AI."
            },
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {"type": "image_url", "image_url": {
                        "url": f"data:image/png;base64,{base64_image}"
                    }}
                ]
            }
        ]
    )
    return response.choices[0].message.content.strip()

# Example
# if __name__ == "__main__":
#     arxiv_id = "0704.0001"
#     query = "How does the paper calculate diphoton production cross-sections?"
#     chunks = load_and_chunk(arxiv_id)
#     top_chunks = retrieve_top_chunks(query, chunks, top_k=5)
#     answer = standalone_answer(query, top_chunks, MODEL_NAME, api_key=os.getenv("OPENAI_API_KEY"))
#
#     print("üîé Answer:")
#     print(answer)
